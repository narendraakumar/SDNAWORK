{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLyhNbSrBkRQ7A7R8eM9F7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narendraakumar/SDNAWORK/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVX5QplgVQ6B",
        "outputId": "b16513d3-55ed-4263-ed16-cfceae689a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: yolov5 in /usr/local/lib/python3.10/dist-packages (7.0.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from yolov5) (3.1.43)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from yolov5) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (1.13.1)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (4.66.6)\n",
            "Requirement already satisfied: ultralytics>=8.0.100 in /usr/local/lib/python3.10/dist-packages (from yolov5) (8.3.28)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.17.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (75.1.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.7.0)\n",
            "Requirement already satisfied: boto3>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (1.35.57)\n",
            "Requirement already satisfied: sahi>=0.11.10 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.11.18)\n",
            "Requirement already satisfied: huggingface-hub>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.24.7)\n",
            "Requirement already satisfied: roboflow>=0.2.29 in /usr/local/lib/python3.10/dist-packages (from yolov5) (1.1.49)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.57 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5) (1.35.57)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5) (0.10.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->yolov5) (4.0.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->yolov5) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->yolov5) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (2024.8.30)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5) (1.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5) (1.2.0)\n",
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5) (2.0.6)\n",
            "Requirement already satisfied: pybboxes==0.1.6 in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5) (0.1.6)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5) (3.1.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5) (8.1.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (3.0.6)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.100->yolov5) (2.0.11)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->yolov5) (2.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.1)\n",
            "Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "Successfully installed opencv-python-4.9.0.80\n",
            "Requirement already satisfied: grad-cam in /usr/local/lib/python3.10/dist-packages (1.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (10.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.20.0+cu121)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.9.0.80)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
            "Requirement already satisfied: rembg in /usr/local/lib/python3.10/dist-packages (2.0.59)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.26.4)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from rembg) (1.20.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (10.4.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.2)\n",
            "Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg) (1.1.12)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.66.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.20.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (1.13.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.32.3)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.60.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (0.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2024.8.30)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->rembg) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->rembg) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torch torchvision opencv-python opencv-python-headless matplotlib yolov5\n",
        "!pip install grad-cam\n",
        "!pip install rembg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install detectron2\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cTYf90AeInj",
        "outputId": "8b9a3c7f-f3ee-45a5-c5c7-1438868d9fe2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  2.5 ; cuda:  cu121\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu121/torch2.5/index.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement detectron2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for detectron2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cci0UuBNeJWd",
        "outputId": "7bd0f17d-48e1-45a6-bc9b-83810726e374"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Using cached PyYAML-5.1.tar.gz (274 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
            "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (24.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7) (2.10.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.1) (4.9.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Vdww_qePdl",
        "outputId": "3322e44e-4d14-4382-a563-767eb634f279"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.5 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone and install Mask2Former\n",
        "!git clone https://github.com/facebookresearch/Mask2Former.git\n",
        "%cd Mask2Former\n",
        "!pip install -U opencv-python\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!pip install -r requirements.txt\n",
        "%cd mask2former/modeling/pixel_decoder/ops\n",
        "!python setup.py build install\n",
        "%cd ../../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "SswXGSK4ea5i",
        "outputId": "017e579b-f60b-4572-f7c6-bd443b7fa493"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mask2Former' already exists and is not an empty directory.\n",
            "/content/Mask2Former\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.9.0.80\n",
            "    Uninstalling opencv-python-4.9.0.80:\n",
            "      Successfully uninstalled opencv-python-4.9.0.80\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sahi 0.11.18 requires opencv-python<=4.9.0.80, but you have opencv-python 4.10.0.84 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opencv-python-4.10.0.84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              },
              "id": "76a66128b8424ffb8f9fc23f66f7f5fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-2wdn2ys4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-2wdn2ys4\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (10.4.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.11)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.12.1)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.24.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.20.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.4.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (4.66.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2024.8.30)\n",
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n",
            "running build\n",
            "running build_py\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "/content/Mask2Former\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "%cd /content/Mask2Former\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "setup_logger(name=\"mask2former\")\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "coco_metadata = MetadataCatalog.get(\"coco_2017_val_panoptic\")\n",
        "\n",
        "# import Mask2Former project\n",
        "from mask2former import add_maskformer2_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B00GcdXzea2v",
        "outputId": "870dbcda-5181-4406-c9be-efd783a58899"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/msdeformattn.py:314: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Mask2Former/mask2former/modeling/pixel_decoder/ops/setup.py build install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz_wCg3zeauU",
        "outputId": "471bb8a8-59b7-402d-f3da-4af8c18ae94e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build\n",
            "running build_py\n",
            "copying mask2former_video/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video\n",
            "copying mask2former_video/video_maskformer_model.py -> build/lib.linux-x86_64-cpython-310/mask2former_video\n",
            "copying mask2former_video/config.py -> build/lib.linux-x86_64-cpython-310/mask2former_video\n",
            "copying mask2former/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former\n",
            "copying mask2former/test_time_augmentation.py -> build/lib.linux-x86_64-cpython-310/mask2former\n",
            "copying mask2former/maskformer_model.py -> build/lib.linux-x86_64-cpython-310/mask2former\n",
            "copying mask2former/config.py -> build/lib.linux-x86_64-cpython-310/mask2former\n",
            "copying mask2former_video/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/utils\n",
            "copying mask2former_video/utils/memory.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/utils\n",
            "copying mask2former_video/modeling/criterion.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/modeling\n",
            "copying mask2former_video/modeling/matcher.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/modeling\n",
            "copying mask2former_video/modeling/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/modeling\n",
            "copying mask2former_video/data_video/augmentation.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video\n",
            "copying mask2former_video/data_video/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video\n",
            "copying mask2former_video/data_video/dataset_mapper.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video\n",
            "copying mask2former_video/data_video/ytvis_eval.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video\n",
            "copying mask2former_video/data_video/build.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video\n",
            "copying mask2former_video/modeling/transformer_decoder/video_mask2former_transformer_decoder.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/transformer_decoder\n",
            "copying mask2former_video/modeling/transformer_decoder/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/transformer_decoder\n",
            "copying mask2former_video/modeling/transformer_decoder/position_encoding.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/transformer_decoder\n",
            "copying mask2former_video/data_video/datasets/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets\n",
            "copying mask2former_video/data_video/datasets/builtin.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets\n",
            "copying mask2former_video/data_video/datasets/ytvis.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets\n",
            "copying mask2former_video/data_video/datasets/ytvis_api/ytvos.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying mask2former_video/data_video/datasets/ytvis_api/ytvoseval.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying mask2former_video/data_video/datasets/ytvis_api/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying mask2former/utils/misc.py -> build/lib.linux-x86_64-cpython-310/mask2former/utils\n",
            "copying mask2former/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/utils\n",
            "copying mask2former/data/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/data\n",
            "copying mask2former/evaluation/instance_evaluation.py -> build/lib.linux-x86_64-cpython-310/mask2former/evaluation\n",
            "copying mask2former/evaluation/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/evaluation\n",
            "copying mask2former/modeling/criterion.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling\n",
            "copying mask2former/modeling/matcher.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling\n",
            "copying mask2former/modeling/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling\n",
            "copying mask2former/data/dataset_mappers/mask_former_semantic_dataset_mapper.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers\n",
            "copying mask2former/data/dataset_mappers/coco_panoptic_new_baseline_dataset_mapper.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers\n",
            "copying mask2former/data/dataset_mappers/mask_former_panoptic_dataset_mapper.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers\n",
            "copying mask2former/data/dataset_mappers/mask_former_instance_dataset_mapper.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers\n",
            "copying mask2former/data/dataset_mappers/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers\n",
            "copying mask2former/data/dataset_mappers/coco_instance_new_baseline_dataset_mapper.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers\n",
            "copying mask2former/data/datasets/register_coco_panoptic_annos_semseg.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/register_mapillary_vistas_panoptic.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/register_ade20k_instance.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/register_ade20k_panoptic.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/register_ade20k_full.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/register_coco_stuff_10k.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/data/datasets/register_mapillary_vistas.py -> build/lib.linux-x86_64-cpython-310/mask2former/data/datasets\n",
            "copying mask2former/modeling/transformer_decoder/maskformer_transformer_decoder.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder\n",
            "copying mask2former/modeling/transformer_decoder/transformer.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder\n",
            "copying mask2former/modeling/transformer_decoder/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder\n",
            "copying mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder\n",
            "copying mask2former/modeling/transformer_decoder/position_encoding.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder\n",
            "copying mask2former/modeling/pixel_decoder/fpn.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/pixel_decoder\n",
            "copying mask2former/modeling/pixel_decoder/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/pixel_decoder\n",
            "copying mask2former/modeling/pixel_decoder/msdeformattn.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/pixel_decoder\n",
            "copying mask2former/modeling/backbone/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/backbone\n",
            "copying mask2former/modeling/backbone/swin.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/backbone\n",
            "copying mask2former/modeling/meta_arch/per_pixel_baseline.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/meta_arch\n",
            "copying mask2former/modeling/meta_arch/__init__.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/meta_arch\n",
            "copying mask2former/modeling/meta_arch/mask_former_head.py -> build/lib.linux-x86_64-cpython-310/mask2former/modeling/meta_arch\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/utils/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/utils/memory.py -> build/bdist.linux-x86_64/egg/mask2former_video/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/video_maskformer_model.py -> build/bdist.linux-x86_64/egg/mask2former_video\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/config.py -> build/bdist.linux-x86_64/egg/mask2former_video\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video/modeling\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/transformer_decoder/video_mask2former_transformer_decoder.py -> build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/transformer_decoder/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/transformer_decoder/position_encoding.py -> build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/criterion.py -> build/bdist.linux-x86_64/egg/mask2former_video/modeling\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/matcher.py -> build/bdist.linux-x86_64/egg/mask2former_video/modeling\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/modeling/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video/modeling\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video/data_video\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets\n",
            "creating build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis_api/ytvos.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis_api/ytvoseval.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis_api/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/builtin.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/datasets/ytvis.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/augmentation.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/__init__.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/dataset_mapper.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/ytvis_eval.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former_video/data_video/build.py -> build/bdist.linux-x86_64/egg/mask2former_video/data_video\n",
            "creating build/bdist.linux-x86_64/egg/mask2former\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/__init__.py -> build/bdist.linux-x86_64/egg/mask2former\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/test_time_augmentation.py -> build/bdist.linux-x86_64/egg/mask2former\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/utils/misc.py -> build/bdist.linux-x86_64/egg/mask2former/utils\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/utils/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/utils\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/data\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers/mask_former_semantic_dataset_mapper.py -> build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers/coco_panoptic_new_baseline_dataset_mapper.py -> build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers/mask_former_panoptic_dataset_mapper.py -> build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers/mask_former_instance_dataset_mapper.py -> build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/dataset_mappers/coco_instance_new_baseline_dataset_mapper.py -> build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_coco_panoptic_annos_semseg.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_mapillary_vistas_panoptic.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_ade20k_instance.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_ade20k_panoptic.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_ade20k_full.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_coco_stuff_10k.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/datasets/register_mapillary_vistas.py -> build/bdist.linux-x86_64/egg/mask2former/data/datasets\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/data/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/data\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/maskformer_model.py -> build/bdist.linux-x86_64/egg/mask2former\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/evaluation\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/evaluation/instance_evaluation.py -> build/bdist.linux-x86_64/egg/mask2former/evaluation\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/evaluation/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/evaluation\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/config.py -> build/bdist.linux-x86_64/egg/mask2former\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/modeling\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder/maskformer_transformer_decoder.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder/transformer.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/transformer_decoder/position_encoding.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/criterion.py -> build/bdist.linux-x86_64/egg/mask2former/modeling\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/matcher.py -> build/bdist.linux-x86_64/egg/mask2former/modeling\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/modeling\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/pixel_decoder/fpn.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/pixel_decoder/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/pixel_decoder/msdeformattn.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/modeling/backbone\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/backbone/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/backbone\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/backbone/swin.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/backbone\n",
            "creating build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/meta_arch/per_pixel_baseline.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/meta_arch/__init__.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch\n",
            "copying build/lib.linux-x86_64-cpython-310/mask2former/modeling/meta_arch/mask_former_head.py -> build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/utils/memory.py to memory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/video_maskformer_model.py to video_maskformer_model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder/video_mask2former_transformer_decoder.py to video_mask2former_transformer_decoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/modeling/transformer_decoder/position_encoding.py to position_encoding.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/modeling/criterion.py to criterion.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/modeling/matcher.py to matcher.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/modeling/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api/ytvos.py to ytvos.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api/ytvoseval.py to ytvoseval.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis_api/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/builtin.py to builtin.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/datasets/ytvis.py to ytvis.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/augmentation.py to augmentation.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/dataset_mapper.py to dataset_mapper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/ytvis_eval.py to ytvis_eval.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former_video/data_video/build.py to build.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/test_time_augmentation.py to test_time_augmentation.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/utils/misc.py to misc.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers/mask_former_semantic_dataset_mapper.py to mask_former_semantic_dataset_mapper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers/coco_panoptic_new_baseline_dataset_mapper.py to coco_panoptic_new_baseline_dataset_mapper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers/mask_former_panoptic_dataset_mapper.py to mask_former_panoptic_dataset_mapper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers/mask_former_instance_dataset_mapper.py to mask_former_instance_dataset_mapper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/dataset_mappers/coco_instance_new_baseline_dataset_mapper.py to coco_instance_new_baseline_dataset_mapper.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_coco_panoptic_annos_semseg.py to register_coco_panoptic_annos_semseg.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_mapillary_vistas_panoptic.py to register_mapillary_vistas_panoptic.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_ade20k_instance.py to register_ade20k_instance.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_ade20k_panoptic.py to register_ade20k_panoptic.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_ade20k_full.py to register_ade20k_full.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_coco_stuff_10k.py to register_coco_stuff_10k.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/datasets/register_mapillary_vistas.py to register_mapillary_vistas.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/data/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/maskformer_model.py to maskformer_model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/evaluation/instance_evaluation.py to instance_evaluation.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/evaluation/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder/maskformer_transformer_decoder.py to maskformer_transformer_decoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder/transformer.py to transformer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py to mask2former_transformer_decoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/transformer_decoder/position_encoding.py to position_encoding.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/criterion.py to criterion.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/matcher.py to matcher.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder/fpn.py to fpn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/pixel_decoder/msdeformattn.py to msdeformattn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/backbone/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/backbone/swin.py to swin.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch/per_pixel_baseline.py to per_pixel_baseline.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mask2former/modeling/meta_arch/mask_former_head.py to mask_former_head.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86eutmgReary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from rembg import remove\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import preprocess_image\n",
        "from torchvision.models import resnet50\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from skimage.filters import sobel\n",
        "from skimage import img_as_float, color\n",
        "from PIL import Image\n",
        "\n",
        "# Load ResNet50 model for Grad-CAM\n",
        "resnet_model = resnet50(pretrained=True)\n",
        "grad_cam = GradCAM(model=resnet_model, target_layers=[resnet_model.layer4[-1]])\n",
        "\n",
        "\n",
        "\n",
        "def remove_background(image_path):\n",
        "    input_image = Image.open(image_path)\n",
        "    bg_removed = remove(input_image)  # Remove background using rembg\n",
        "    bg_removed_np = np.array(bg_removed)  # Convert to numpy array for OpenCV processing\n",
        "    return bg_removed_np\n",
        "\n",
        "def remove_background1(image_path):\n",
        "    # Load the original image\n",
        "    input_image = Image.open(image_path).convert(\"RGBA\")\n",
        "    # Use rembg to remove background\n",
        "    bg_removed = remove(input_image)\n",
        "\n",
        "    # Convert both images to numpy arrays\n",
        "    bg_removed_np = np.array(bg_removed)  # Hero product with transparent background\n",
        "    original_np = np.array(input_image)    # Original image\n",
        "\n",
        "    # Create a mask from the alpha channel of bg_removed_np (non-zero where hero product is)\n",
        "    mask = bg_removed_np[:, :, 3] > 0  # Alpha channel\n",
        "\n",
        "    # Use the mask to isolate the background from the original image\n",
        "    background_only = original_np.copy()\n",
        "    background_only[mask] = [0, 0, 0, 0]  # Make the hero product area transparent (white background)\n",
        "    original_np[mask] = [0,0,0,0]\n",
        "    # rgb_image = original_np.convert(\"RGB\")\n",
        "    rgb_image = original_np[:, :, :3]\n",
        "\n",
        "\n",
        "    return bg_removed_np, background_only, rgb_image\n",
        "\n",
        "\n",
        "def calculate_prominence(bg_removed_np):\n",
        "    hero_product_mask = cv2.cvtColor(bg_removed_np, cv2.COLOR_RGBA2GRAY)\n",
        "    hero_product_mask = (hero_product_mask > 0).astype(np.uint8)\n",
        "\n",
        "    # Calculate area ratio (prominence) as a fraction of the total image area\n",
        "    hero_product_area = np.sum(hero_product_mask)\n",
        "    img_area = hero_product_mask.shape[0] * hero_product_mask.shape[1]\n",
        "    prominence_ratio = hero_product_area / img_area\n",
        "\n",
        "    prominence_score = 1 + (prominence_ratio * 9)\n",
        "\n",
        "    return round(prominence_score, 2)\n",
        "\n",
        "def is_centered(hero_product_mask, threshold=0.1):\n",
        "    # Calculate bounding box of non-zero pixels in mask\n",
        "    coords = cv2.findNonZero(hero_product_mask)\n",
        "    x, y, w, h = cv2.boundingRect(coords)  # Bounding box around hero product\n",
        "    img_center = (hero_product_mask.shape[1] / 2, hero_product_mask.shape[0] / 2)\n",
        "    product_center = (x + w / 2, y + h / 2)\n",
        "    offset = np.linalg.norm(np.array(img_center) - np.array(product_center))\n",
        "    max_offset = threshold * np.linalg.norm(img_center)\n",
        "    return offset <= max_offset,(x, y, x+w, y+h)\n",
        "\n",
        "def is_centered_score(hero_product_mask,im=None, threshold=0.1):\n",
        "    coords = cv2.findNonZero(hero_product_mask)\n",
        "    x, y, w, h = cv2.boundingRect(coords)\n",
        "    print(\"hero product bounding box\",x,y,w,h)\n",
        "    img_center = (int(hero_product_mask.shape[1] / 2), int(hero_product_mask.shape[0] / 2))\n",
        "    product_center = (int((x + x+ w) / 2), int((y +y+ h )/ 2))\n",
        "    # print(\"hero_product_mask shape\",hero_product_mask.shape,\"image shape\",im.shape)\n",
        "    # print(\"image center\",img_center,\"product center\",product_center)\n",
        "    # point_radius = 6  # Small radius for the point\n",
        "    # point_color = (0, 0, 255)  # Red color in BGR format\n",
        "    # cv2.circle(im, img_center, point_radius, point_color, thickness=1)\n",
        "    # point_color = (255, 0, 0)\n",
        "    # cv2.circle(im, product_center, point_radius, point_color, thickness=1)\n",
        "    # cv2.imwrite(\"/content/i.png\",im)\n",
        "    offset = np.linalg.norm(np.array(img_center) - np.array(product_center))\n",
        "    max_offset = np.linalg.norm(np.array(img_center)- np.array([int(w/2),int(h/2)]))\n",
        "\n",
        "    # Calculate the score based on offset, mapping to a 1-10 scale\n",
        "    print(\"max offset:\",max_offset,\"offset\",offset)\n",
        "    if offset <= max_offset*threshold:\n",
        "        print(offset/max_offset)\n",
        "        score = 10.0\n",
        "    else:\n",
        "        # offset_ratio = min(offset / max_offset, 2)  # Cap the ratio to prevent excessive drop\n",
        "        # score = max(1, 10 - (offset_ratio - 1) * 9)  # Linear drop after threshold\n",
        "        score = np.round(((1-offset/max_offset))*10)\n",
        "\n",
        "    return score\n",
        "\n",
        "# Generate Grad-CAM heatmap for focus analysis\n",
        "# def generate_gradcam_heatmap(image, target_class=207):  # Example target class\n",
        "#     preprocessed_image = preprocess_image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     targets = [ClassifierOutputTarget(target_class)]\n",
        "#     mask = grad_cam(input_tensor=preprocessed_image, targets=targets)[0]\n",
        "#     heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "#     return cv2.addWeighted(cv2.resize(image, (mask.shape[1], mask.shape[0])), 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "def generate_gradcam_heatmap(image, model, target_class=207):  # Example target class\n",
        "    # Preprocess the input image\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    from pytorch_grad_cam.utils.image import preprocess_image\n",
        "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "    from pytorch_grad_cam import GradCAM\n",
        "    preprocessed_image = preprocess_image(\n",
        "        cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    targets = [ClassifierOutputTarget(target_class)]\n",
        "\n",
        "    # Generate Grad-CAM mask\n",
        "    with GradCAM(model=model, target_layers=[model.layer4[-1]]) as cam:\n",
        "        mask = cam(input_tensor=preprocessed_image, targets=targets)[0]\n",
        "\n",
        "    # Create color heatmap\n",
        "    color_heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    color_heatmap = cv2.addWeighted(\n",
        "        cv2.resize(image, (mask.shape[1], mask.shape[0])), 0.6,\n",
        "        color_heatmap, 0.4, 0\n",
        "    )\n",
        "\n",
        "    # Create grayscale heatmap\n",
        "    grayscale_heatmap = np.uint8(255 * mask)  # Normalize mask to 0-255 range\n",
        "\n",
        "    return color_heatmap, grayscale_heatmap\n",
        "\n",
        "\n",
        "# Generate saliency map using Sobel edge detection\n",
        "def generate_saliency_map(image):\n",
        "    image_gray = color.rgb2gray(img_as_float(image))\n",
        "    saliency_map = sobel(image_gray)\n",
        "    saliency_map = (saliency_map * 255).astype(\"uint8\")\n",
        "    return saliency_map\n",
        "\n",
        "# Grading based on criteria\n",
        "def grade_image(centered, prominent, focus):\n",
        "    grade = 0\n",
        "    grade += 3 if centered else 0\n",
        "    grade += 3 if prominent else 0\n",
        "    grade += 4 if focus else 0\n",
        "    print(\"centered\", \"prominent\", \"focus\",centered, prominent, focus)  # Focus gets a higher weight\n",
        "    return grade\n",
        "\n",
        "def calculate_focus(bbox, gradcam_heatmap, saliency_map, overlap_threshold=0.6):\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "\n",
        "    # Crop to bounding box region\n",
        "    gradcam_crop = gradcam_heatmap[y_min:y_max, x_min:x_max]\n",
        "    saliency_crop = saliency_map[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Convert to binary maps (1 if pixel intensity > threshold, else 0)\n",
        "    gradcam_binary = (gradcam_crop > 128).astype(np.uint8)\n",
        "    saliency_binary = (saliency_crop > 128).astype(np.uint8)\n",
        "\n",
        "    # Calculate overlap (intersection of the two binary masks)\n",
        "    overlap = cv2.bitwise_and(gradcam_binary, saliency_binary)\n",
        "    overlap_area = np.sum(overlap)\n",
        "\n",
        "    # Calculate focus percentage within the bounding box\n",
        "    bbox_area = gradcam_binary.shape[0] * gradcam_binary.shape[1]\n",
        "    focus_ratio = overlap_area / float(bbox_area)\n",
        "\n",
        "    # Determine if focus is sufficient\n",
        "    is_focused = focus_ratio >= overlap_threshold\n",
        "    return is_focused, focus_ratio\n",
        "\n",
        "\n",
        "# Main processing and grading function\n",
        "def main(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Background removal to extract hero product\n",
        "    bg_removed_np, background_only,ip = remove_background1(image_path)\n",
        "    # Focus Check - Grad-CAM\n",
        "    # plt.imshow(cv2.cvtColor(bg_removed_np, cv2.COLOR_BGR2RGB))\n",
        "    # plt.title(\"bg removed np\")\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    # plt.imshow(cv2.cvtColor(ip, cv2.COLOR_BGRA2RGBA))\n",
        "    # plt.title(\"Remaining Background Only\")\n",
        "    # plt.show()\n",
        "\n",
        "    # Calculate prominence\n",
        "    prominence_ratio = calculate_prominence(bg_removed_np)\n",
        "    prominent = prominence_ratio >= 0.3  # Example threshold for prominence\n",
        "    print(\"Prominent (occupies enough area):\", prominent)\n",
        "\n",
        "    # Centering Check\n",
        "    hero_product_mask = cv2.cvtColor(bg_removed_np, cv2.COLOR_RGBA2GRAY)\n",
        "    hero_product_mask = (hero_product_mask > 0).astype(np.uint8)\n",
        "    # centered,bbox = is_centered(hero_product_mask)\n",
        "    centered_score = is_centered_score(hero_product_mask)\n",
        "    # print(\"Centered:\", centered)\n",
        "    print(\"Centered score:\", centered_score)\n",
        "\n",
        "    # Focus Check - Grad-CAM\n",
        "    # import torchvision.models as models\n",
        "    # import torch\n",
        "\n",
        "    # Load a pre-trained ResNet model\n",
        "    # model = models.resnet50(pretrained=True)\n",
        "    # model.eval()\n",
        "    # gradcam_result,grayscale_heatmap = generate_gradcam_heatmap(image,model)\n",
        "    # plt.imshow(cv2.cvtColor(gradcam_result, cv2.COLOR_BGR2RGB))\n",
        "    # plt.title(\"Grad-CAM Focus Map\")\n",
        "    # plt.show()\n",
        "\n",
        "    # Saliency Check\n",
        "    # saliency_map = generate_saliency_map(image)\n",
        "    # plt.imshow(saliency_map, cmap='gray')\n",
        "    # plt.title(\"Saliency Map\")\n",
        "    # plt.show()\n",
        "\n",
        "    # Decide if focus is on hero product by inspecting saliency overlaps (for demo assume True)\n",
        "    # focus = True\n",
        "    # focus, focus_ratio = calculate_focus(bbox, grayscale_heatmap, saliency_map)\n",
        "    # print(\"Focus on hero product:\", focus, \"with overlap ratio:\", focus_ratio)\n",
        "\n",
        "    return centered_score, prominence_ratio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsjRmU7gV-5V",
        "outputId": "f066e9d1-9357-4249-a69a-86174e822d89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "add_deeplab_config(cfg)\n",
        "add_maskformer2_config(cfg)\n",
        "cfg.merge_from_file(\"configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml\")\n",
        "cfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/panoptic/maskformer2_swin_large_IN21k_384_bs16_100ep/model_final_f07440.pkl'\n",
        "cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON = True\n",
        "cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON = True\n",
        "cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON = True\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "bbDNHRZVd754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeYPlELJe1vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_segmentation(image_path):\n",
        "  im = cv2.imread(image_path)\n",
        "  outputs = predictor(im)\n",
        "  # Show panoptic/instance/semantic predictions:\n",
        "  v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
        "  panoptic_result = v.draw_panoptic_seg(outputs[\"panoptic_seg\"][0].to(\"cpu\"), outputs[\"panoptic_seg\"][1]).get_image()\n",
        "\n",
        "  panoptic_seg, segments_info = outputs[\"panoptic_seg\"]\n",
        "\n",
        "  # Convert panoptic segmentation to a NumPy array for processing\n",
        "  panoptic_seg_np = panoptic_seg.to(\"cpu\").numpy()\n",
        "\n",
        "  # Draw bounding boxes around each unique segment\n",
        "  im_with_boxes = im.copy()\n",
        "  for segment in segments_info:\n",
        "      segment_id = segment[\"id\"]\n",
        "      # Create a mask for the current segment\n",
        "      mask = (panoptic_seg_np == segment_id).astype(np.uint8)\n",
        "\n",
        "      # Find contours of the segment mask to get the bounding box\n",
        "      contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      for contour in contours:\n",
        "          # Get bounding box coordinates for the contour\n",
        "          x, y, w, h = cv2.boundingRect(contour)\n",
        "          # Draw the bounding box on the image\n",
        "          cv2.rectangle(im_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green bounding box\n",
        ""
      ],
      "metadata": {
        "id": "3TZ7yR7Be1sa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wall_class_id = 131\n",
        "for idx, class_name in enumerate(coco_metadata.stuff_classes):\n",
        "    if class_name.lower() == \"chair\":\n",
        "        wall_class_id = idx\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "def mask_except_wall_with_transparency(image, outputs, wall_class_id):\n",
        "    \"\"\"Keeps only the wall class opaque and makes everything else transparent.\"\"\"\n",
        "    semantic_segmentation = outputs[\"sem_seg\"].argmax(0).cpu().numpy()\n",
        "\n",
        "    wall_mask = semantic_segmentation == wall_class_id\n",
        "\n",
        "    transparent_image = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
        "\n",
        "    transparent_image[..., :3] = image\n",
        "    transparent_image[..., 3] = np.where(wall_mask, 255, 0)\n",
        "    # black_image = np.zeros_like(image)\n",
        "\n",
        "    # black_image[wall_mask] = image[wall_mask]\n",
        "\n",
        "    return transparent_image\n",
        "\n",
        "masked_transparent_image = mask_except_wall_with_transparency(im, outputs, wall_class_id)\n",
        "\n",
        "cv2_imshow(masked_transparent_image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "eFL-rchoe1kN",
        "outputId": "b0f048de-a70f-4684-83d2-cef408b68770"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'im' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c76824212fb8>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransparent_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmasked_transparent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_except_wall_with_transparency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwall_class_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_transparent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'im' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def load_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image_rgb\n",
        "\n",
        "def approximate_wall_area(image, lower_bound=(0, 0, 0), upper_bound=(255, 255, 255)):\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    mask = cv2.inRange(hsv_image, np.array(lower_bound), np.array(upper_bound))\n",
        "\n",
        "    wall_area = cv2.bitwise_and(image, image, mask=mask)\n",
        "    return wall_area\n",
        "\n",
        "def extract_dominant_colors(image, num_colors=3):\n",
        "    flat_image = image.reshape(-1, 3)\n",
        "\n",
        "    flat_image = flat_image[np.any(flat_image != [0, 0, 0], axis=1)]\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_colors)\n",
        "    kmeans.fit(flat_image)\n",
        "    colors = kmeans.cluster_centers_\n",
        "\n",
        "    label_counts = Counter(kmeans.labels_)\n",
        "    total_count = sum(label_counts.values())\n",
        "\n",
        "    color_proportions = {tuple(map(int, colors[i])): count / total_count for i, count in label_counts.items()}\n",
        "    return color_proportions\n",
        "\n",
        "def plot_colors(color_proportions):\n",
        "    sorted_colors = dict(sorted(color_proportions.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    plt.figure(figsize=(8, 2))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    bar = np.zeros((50, 300, 3), dtype=\"uint8\")\n",
        "    start_x = 0\n",
        "\n",
        "    for color, proportion in sorted_colors.items():\n",
        "        end_x = start_x + int(proportion * 300)\n",
        "        bar[:, start_x:end_x] = color\n",
        "        start_x = end_x\n",
        "\n",
        "    plt.imshow(bar)\n",
        "    plt.show()\n",
        "\n",
        "    for color, proportion in sorted_colors.items():\n",
        "        color = \"{0}:{1}:{2}\".format(color[0], color[1], color[2])\n",
        "        print(f\"Color {color}: {proportion:.2%}\")\n",
        "\n",
        "def analyze_wall_colors(image_path, num_colors=3):\n",
        "    image = load_image(image_path)\n",
        "    wall_area = approximate_wall_area(image)\n",
        "\n",
        "    color_proportions = extract_dominant_colors(wall_area, num_colors)\n",
        "\n",
        "    plot_colors(color_proportions)\n",
        "\n",
        "# analyze_wall_colors(\"/content/masked_transparent_image.png\", num_colors=7)"
      ],
      "metadata": {
        "id": "rChgO-QOfkhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "new_directory = \"/content\"\n",
        "os.chdir(new_directory)\n",
        "!unzip /content/testdatawayfair.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-2HFJrCoXyy_",
        "outputId": "805d3701-f8e8-47df-99fd-b76657b3292d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/testdatawayfair.zip\n",
            "replace __MACOSX/._testdatawayfair? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "raKB9UCXzCO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def get_all_image_paths(main_folder):\n",
        "    image_paths = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(main_folder):\n",
        "        current_folder_images = []\n",
        "\n",
        "        for ext in ('*.png', '*.jpeg', '*.jpg'):\n",
        "            current_folder_images.extend(glob.glob(os.path.join(root, ext)))\n",
        "\n",
        "        if current_folder_images:\n",
        "            folder_name = os.path.relpath(root, main_folder)\n",
        "            image_paths[folder_name] = current_folder_images\n",
        "\n",
        "    return image_paths\n",
        "all_imgs = get_all_image_paths(\"/content/testdatawayfair\")"
      ],
      "metadata": {
        "id": "fiZqZf-SYiWk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_imgs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSNB9PEugg3W",
        "outputId": "1a85170e-c262-4f85-9aaa-b19a138bdec3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1': ['/content/testdatawayfair/1/imgpsh_fullsize_anim (13)_2.png',\n",
              "  '/content/testdatawayfair/1/imgpsh_fullsize_anim (13).png',\n",
              "  '/content/testdatawayfair/1/image_2-8.png',\n",
              "  '/content/testdatawayfair/1/silo.png'],\n",
              " '5': ['/content/testdatawayfair/5/silo.png',\n",
              "  '/content/testdatawayfair/5/image_4K6.png'],\n",
              " '6': ['/content/testdatawayfair/6/silo.png',\n",
              "  '/content/testdatawayfair/6/image_wDT.jpeg'],\n",
              " '7': ['/content/testdatawayfair/7/silo.png',\n",
              "  '/content/testdatawayfair/7/imgpsh_fullsize_anim (7).jpeg'],\n",
              " '2': ['/content/testdatawayfair/2/silo.png',\n",
              "  '/content/testdatawayfair/2/image_u0e_1_2k.png'],\n",
              " '4': ['/content/testdatawayfair/4/unnamed1-removebg-preview.png',\n",
              "  '/content/testdatawayfair/4/image_NT2.jpeg',\n",
              "  '/content/testdatawayfair/4/image_yMs.jpeg',\n",
              "  '/content/testdatawayfair/4/image_jWp.jpeg',\n",
              "  '/content/testdatawayfair/4/image_5cl.jpeg'],\n",
              " '3': ['/content/testdatawayfair/3/silo.png',\n",
              "  '/content/testdatawayfair/3/image_6Ww.jpeg',\n",
              "  '/content/testdatawayfair/3/image_YZM.jpeg',\n",
              "  '/content/testdatawayfair/3/image_83q.jpeg',\n",
              "  '/content/testdatawayfair/3/image_h_j.jpeg',\n",
              "  '/content/testdatawayfair/3/image_Dkz.jpeg']}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sku in all_imgs:\n",
        "  for img_path in all_imgs[sku]:\n",
        "    print(img_path)\n",
        "    centered_score, prominence_ratio = main(img_path)\n",
        "\n",
        "    print(centered_score,prominence_ratio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQe4G3T-Yo_5",
        "outputId": "27370192-5ea1-4e9c-90eb-6b0a46372fa6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/testdatawayfair/1/imgpsh_fullsize_anim (13)_2.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 118 717 1757 1640\n",
            "max offset: 589.1858789889656 offset 264.78104161740885\n",
            "Centered score: 6.0\n",
            "6.0 4.32\n",
            "/content/testdatawayfair/1/imgpsh_fullsize_anim (13).png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 510 732 1759 1642\n",
            "max offset: 728.0618105628121 offset 190.64626930522402\n",
            "Centered score: 7.0\n",
            "7.0 3.78\n",
            "/content/testdatawayfair/1/image_2-8.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 189 275 660 626\n",
            "max offset: 269.675731203236 offset 76.32168761236873\n",
            "Centered score: 7.0\n",
            "7.0 3.83\n",
            "/content/testdatawayfair/1/silo.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 517 759 1723 1598\n",
            "max offset: 756.46149935076 offset 194.50449866262733\n",
            "Centered score: 7.0\n",
            "7.0 3.67\n",
            "/content/testdatawayfair/5/silo.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 88 106 760 766\n",
            "max offset: 130.83195328359201 offset 16.15549442140351\n",
            "Centered score: 9.0\n",
            "9.0 3.83\n",
            "/content/testdatawayfair/5/image_4K6.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 218 848 672\n",
            "max offset: 196.7739820199815 offset 97.50897394599124\n",
            "Centered score: 5.0\n",
            "5.0 2.43\n",
            "/content/testdatawayfair/6/silo.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 31 41 579 572\n",
            "max offset: 64.35060217278468 offset 15.524174696260024\n",
            "Centered score: 8.0\n",
            "8.0 4.75\n",
            "/content/testdatawayfair/6/image_wDT.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 249 900 680\n",
            "max offset: 182.8332573685652 offset 98.85848471426213\n",
            "Centered score: 5.0\n",
            "5.0 3.25\n",
            "/content/testdatawayfair/7/silo.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 118 233 712 640\n",
            "max offset: 247.38633753705963 offset 55.90169943749474\n",
            "Centered score: 8.0\n",
            "8.0 4.38\n",
            "/content/testdatawayfair/7/imgpsh_fullsize_anim (7).jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 338 651 1346 1186\n",
            "max offset: 555.8435031553396 offset 220.38375620721234\n",
            "Centered score: 6.0\n",
            "6.0 3.91\n",
            "/content/testdatawayfair/2/silo.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 8 7 2164 2486\n",
            "max offset: 10.63014581273465 offset 0.0\n",
            "0.0\n",
            "Centered score: 10\n",
            "10 6.39\n",
            "/content/testdatawayfair/2/image_u0e_1_2k.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 275 582 1773 1404\n",
            "max offset: 350.32556286973977 offset 293.88603233226314\n",
            "Centered score: 2.0\n",
            "2.0 3.92\n",
            "/content/testdatawayfair/4/unnamed1-removebg-preview.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 50 0 462 305\n",
            "max offset: 25.079872407968907 offset 25.079872407968907\n",
            "Centered score: 0.0\n",
            "0.0 3.51\n",
            "/content/testdatawayfair/4/image_NT2.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 81 47 740 779\n",
            "max offset: 137.84411485442533 offset 68.18357573492314\n",
            "Centered score: 5.0\n",
            "5.0 2.81\n",
            "/content/testdatawayfair/4/image_yMs.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 78 49 742 780\n",
            "max offset: 133.13526955694348 offset 61.07372593840988\n",
            "Centered score: 5.0\n",
            "5.0 2.98\n",
            "/content/testdatawayfair/4/image_jWp.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 78 50 815 796\n",
            "max offset: 112.72089424769483 offset 66.2872536767062\n",
            "Centered score: 4.0\n",
            "4.0 2.94\n",
            "/content/testdatawayfair/4/image_5cl.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 81 54 774 779\n",
            "max offset: 129.25169244539896 offset 64.8459713474939\n",
            "Centered score: 5.0\n",
            "5.0 2.9\n",
            "/content/testdatawayfair/3/silo.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 1 539 802 958\n",
            "max offset: 271.0 offset 268.00186566514793\n",
            "Centered score: 0.0\n",
            "0.0 1.6\n",
            "/content/testdatawayfair/3/image_6Ww.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 212 0 812 1024\n",
            "max offset: 106.0 offset 106.0\n",
            "Centered score: 0.0\n",
            "0.0 5.38\n",
            "/content/testdatawayfair/3/image_YZM.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 38 1024 986\n",
            "max offset: 19.0 offset 19.0\n",
            "Centered score: 0.0\n",
            "0.0 5.38\n",
            "/content/testdatawayfair/3/image_83q.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 42 781 982\n",
            "max offset: 123.79418403139947 offset 123.79418403139947\n",
            "Centered score: 0.0\n",
            "0.0 5.16\n",
            "/content/testdatawayfair/3/image_h_j.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 237 41 787 983\n",
            "max offset: 120.83873551142449 offset 119.68291440301745\n",
            "Centered score: 0.0\n",
            "0.0 5.36\n",
            "/content/testdatawayfair/3/image_Dkz.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 163 624 861\n",
            "max offset: 216.15735009478627 offset 215.77998053572995\n",
            "Centered score: 0.0\n",
            "0.0 1.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LYIg3bw-82B",
        "outputId": "dc879ab1-e43e-44fc-a0b5-afe8196b4aa1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "from openpyxl.drawing.image import Image as OpenpyxlImage\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "wb = openpyxl.Workbook()\n",
        "ws = wb.active\n",
        "ws.title = \"Product Scores\"\n",
        "\n",
        "headers = [\"Product/SKU\", \"Image\",\"Centering Score\", \"Prominence Score\", \"Wall Color Range\", \"Color Palette Analysis Score\"]\n",
        "# Set font for the header row\n",
        "header_font = Font(bold=True, size=16)  # Larger, bold font for headers\n",
        "data_font = Font(size=12)  # Font size for data cells\n",
        "\n",
        "# Set column widths for better visibility\n",
        "column_widths = [16, 14, 15, 15, 15, 15]  # Adjust as necessary\n",
        "\n",
        "# Write headers with styling\n",
        "row_num = 1\n",
        "for col_num, (header, width) in enumerate(zip(headers, column_widths), start=1):\n",
        "    cell = ws.cell(row=1, column=col_num, value=header)\n",
        "    cell.font = header_font  # Apply header font style\n",
        "    cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)  # Center align headers with wrap\n",
        "    ws.column_dimensions[get_column_letter(col_num)].width = width  # Set column width\n",
        "\n",
        "# Set the height of the header row to accommodate wrapped text\n",
        "ws.row_dimensions[row_num].height = 40\n",
        "\n",
        "row_height = 60\n",
        "\n",
        "for i,sku in enumerate(all_imgs):\n",
        "  for j,img_path in enumerate(all_imgs[sku]):\n",
        "    if img_path.split(\"/\")[-1].split(\".\")[0].endswith(\"silo\"):\n",
        "      continue\n",
        "    row_num+=1\n",
        "    print(img_path)\n",
        "    centered_score, prominence_ratio = main(img_path)\n",
        "    ws.cell(row=row_num, column=1, value=i)\n",
        "    ws.cell(row=row_num, column=3, value=centered_score)\n",
        "    ws.cell(row=row_num, column=4, value=prominence_ratio)\n",
        "    ws.cell(row=row_num, column=5, value=0)\n",
        "    ws.cell(row=row_num, column=6, value=0)\n",
        "        # Style data cells\n",
        "    for col in range(1, 7):\n",
        "        cell = ws.cell(row=row_num, column=col)\n",
        "        cell.font = data_font  # Apply font size\n",
        "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")  # Center align data\n",
        "    ws.row_dimensions[row_num].height = row_height\n",
        "\n",
        "    if img_path:\n",
        "        try:\n",
        "            img = OpenpyxlImage(img_path)\n",
        "            img.height = 80\n",
        "            img.width = 80\n",
        "            img.anchor = f\"{get_column_letter(2)}{row_num}\"  # Column \"F\" for images\n",
        "            ws.add_image(img)\n",
        "            # ws.add_image(img, f\"{get_column_letter(2)}{row_num}\")\n",
        "            cell = ws.cell(row=row_num, column=2)\n",
        "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding image for {i}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "output_excel_path = \"product_scores_with_images.xlsx\"\n",
        "wb.save(output_excel_path)\n",
        "\n",
        "print(f\"Excel file with images saved as {output_excel_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J81aVoo7--4T",
        "outputId": "3c57b79a-6b43-43ea-abe2-890af54ae288"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/testdatawayfair/1/imgpsh_fullsize_anim (13)_2.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 118 717 1757 1640\n",
            "max offset: 589.1858789889656 offset 264.78104161740885\n",
            "Centered score: 6.0\n",
            "/content/testdatawayfair/1/imgpsh_fullsize_anim (13).png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 510 732 1759 1642\n",
            "max offset: 728.0618105628121 offset 190.64626930522402\n",
            "Centered score: 7.0\n",
            "/content/testdatawayfair/1/image_2-8.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 189 275 660 626\n",
            "max offset: 269.675731203236 offset 76.32168761236873\n",
            "Centered score: 7.0\n",
            "/content/testdatawayfair/5/image_4K6.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 218 848 672\n",
            "max offset: 196.7739820199815 offset 97.50897394599124\n",
            "Centered score: 5.0\n",
            "/content/testdatawayfair/6/image_wDT.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 249 900 680\n",
            "max offset: 182.8332573685652 offset 98.85848471426213\n",
            "Centered score: 5.0\n",
            "/content/testdatawayfair/7/imgpsh_fullsize_anim (7).jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 338 651 1346 1186\n",
            "max offset: 555.8435031553396 offset 220.38375620721234\n",
            "Centered score: 6.0\n",
            "/content/testdatawayfair/2/image_u0e_1_2k.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 275 582 1773 1404\n",
            "max offset: 350.32556286973977 offset 293.88603233226314\n",
            "Centered score: 2.0\n",
            "/content/testdatawayfair/4/unnamed1-removebg-preview.png\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 50 0 462 305\n",
            "max offset: 25.079872407968907 offset 25.079872407968907\n",
            "Centered score: 0.0\n",
            "/content/testdatawayfair/4/image_NT2.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 81 47 740 779\n",
            "max offset: 137.84411485442533 offset 68.18357573492314\n",
            "Centered score: 5.0\n",
            "/content/testdatawayfair/4/image_yMs.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 78 49 742 780\n",
            "max offset: 133.13526955694348 offset 61.07372593840988\n",
            "Centered score: 5.0\n",
            "/content/testdatawayfair/4/image_jWp.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 78 50 815 796\n",
            "max offset: 112.72089424769483 offset 66.2872536767062\n",
            "Centered score: 4.0\n",
            "/content/testdatawayfair/4/image_5cl.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 81 54 774 779\n",
            "max offset: 129.25169244539896 offset 64.8459713474939\n",
            "Centered score: 5.0\n",
            "/content/testdatawayfair/3/image_6Ww.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 212 0 812 1024\n",
            "max offset: 106.0 offset 106.0\n",
            "Centered score: 0.0\n",
            "/content/testdatawayfair/3/image_YZM.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 38 1024 986\n",
            "max offset: 19.0 offset 19.0\n",
            "Centered score: 0.0\n",
            "/content/testdatawayfair/3/image_83q.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 42 781 982\n",
            "max offset: 123.79418403139947 offset 123.79418403139947\n",
            "Centered score: 0.0\n",
            "/content/testdatawayfair/3/image_h_j.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 237 41 787 983\n",
            "max offset: 120.83873551142449 offset 119.68291440301745\n",
            "Centered score: 0.0\n",
            "/content/testdatawayfair/3/image_Dkz.jpeg\n",
            "Prominent (occupies enough area): True\n",
            "hero product bounding box 0 163 624 861\n",
            "max offset: 216.15735009478627 offset 215.77998053572995\n",
            "Centered score: 0.0\n",
            "Excel file with images saved as product_scores_with_images.xlsx\n"
          ]
        }
      ]
    }
  ]
}